# 学习笔记\_王在田

### Fusion of Facial Expressions and EEG for Multimodal Emotion Recognition

作者单位：华南师范大学，软件学院
时间：2017.9
发表于：https://www.hindawi.com Computational Intelligence and Neuroscience （sci 中科院四区）（已停刊）
被引用：108
数据集：私人数据集
多模态：EEG + facial expression

1. **EEG:** SVM 分类器（机器学习、有监督学习）(happiness, neutral, sadness, and fear)(strong, ordinary, and weak)
2. **facial expression:** 前馈神经网络分类器 (happiness, neutral, sadness, and fear)
3. **fusion:** 决策树 a sum rule or a production rule
   ![](./images/2023-10-07-11-49-25.png) 1. sum rule: calculate the sum of the face expression classifier scores S~1~ and EEG classifier scores S~2~. 2. production rule: 下图中的 P(r~1~,r~2~)中的 r~1~来自表情识别，r~2~来自 EEG
   <img src=./images/2023-10-07-14-20-39.png style="zoom:25%;" />

### Cross-Subject Multimodal Emotion Recognition Based on Hybrid Fusion

作者单位：Loughborough University London
时间：2020.9
发表于：IEEE Access（sci 中科院三区）
被引用：79
数据集：训练时使用公开数据集 Cohn-Kanade+人脸数据集、Radboud Faces 数据库、FacesDB 和 AffectNet。测试时使用公开数据集 LUMED-2 和 DEAP（多模态情感数据集）
多模态：facial expressions, galvanic skin response (GSR) and electroencephalogram (EEG)

1. **facial expression:** InceptionResnetV2
2. **GSR(皮肤电反应)+EEG:** InceptionResnetV2
3. **EEG:** InceptionResnetV2
4. **fusion:** 决策树。 来自表情识别的[P~F~]向量值如果超过了预先设定的阈值 0.9，则采信[P~F~]中的最大情绪。如果没有超过阈值，则比较[P~F~]向量中前两位的情感与 valence-arousal 的距离。
   ![](./images/2023-10-07-15-09-36.png)

有意思的句子：
1.We consider not only the dominant emotion (i.e., the one with the maximum probability), but also the emotion with the second highest probability.

### Video-Based Multimodal Spontaneous Emotion Recognition Using Facial Expressions and Physiological Signals

时间：2022
发表于：CVPR(Conference on Computer Vision and Pattern Recognition)
被引用：8
多模态：facial expressions, physiological signals(HRV)
![](./images/2023-10-18-12-25-09.png)
In this paper the physiological parameters are measured from facial video recordings based on imaging photoplethysmography(iPPG) principal.

1. **facial expression:** networkXception
2. **Physiological Signals:** imaging photoplethysmography(iPPG)
3. **fusion:** 没说清楚

Most existing studies have examined the use of facial expressions and physiological cues separately[12, 24, 31,38, 49]. However, little attention has been paid to a fusion between these two modalities[8, 18, 51].

### A multimodal emotion recognition method based on facial expressions and electroencephalography

时间：2021.9
发表于：Biomedical Signal Processing and Control（生物学期刊）（sci 中科院二区）
作者单位：南开大学人工智能学院
数据集：私人数据集
多模态：EEG + 表情识别(happy sad neutral fear)

### A Novel Emotion Recognition Method Based on the Feature Fusion of Single-Lead EEG and ECG Signals

时间：2023
发表于：IEEE Internet of Things Journal（sci 中科院一区）
作者单位：广东工业大学
数据集：私人数据集
多模态：单通道 EEG + ECG => 情绪(happy relaxed sad scared)
![](./images/2023-10-25-15-58-41.png)
![](./images/2023-10-25-15-58-59.png)
The algorithm for recognition has to be lightweight enough to be embedded in our device to implement edge computing.

LSTM-MLP feature fusion model and the CNN model are used for emotion recognition and classification of multimodal features based on EEG features and HRV features, respectively.

## VLP(Vision-and-Language Pre-training)领域的论文：

### Learning transferable visual models from natural language supervision (CLIP)

作者单位：Open AI
时间：2021
被引用：8703
![](./images/2023-10-17-23-11-47.png)

### An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (VIT)

作者单位：Google
时间：2020
被引用：22968

![](./images/2023-10-17-17-00-20.png)
patch embedding + position embedding
从 BERT 中借鉴来的分类字符 cls。我们认为他能够从别的 embedding 中学到有用的信息，从而只需要根据他的输出做一个最后的判断。

1. transformer 自注意力的时候两两相互 o(N^2^)，那么序列长度一般几百上千（BERT 中为 512）
2. nlp 那边用无监督，视觉还是有监督比较多。
3. 在小数据集上，ViT 不如 ResNet，这是因为归纳偏执（inductive biases）。
4. **patch embedding**：每个 patch 通过一个线性投射层（其实就是全连接层 E）得到一个特征。
   224x224 的图片 ---> 196 个 16x16 的 patch，维度为 16x16x3=768
   196x768 的输入 来到了 768x768(D)的全连接层 E ---> 196x768 + cls token ---> 197x768 （patch embedding 结束）
   **结果**：197 个 token，每个 token 的维度为 768
5. position embedding：位置编码
   有一个表，每一行表示一个位置向量 1x768，将位置向量+patch embedding ---> 197x768（预处理 结束）
6. 文章最后的消融实验探究了几种给位置编码的情况。他们差不多
   1-dimensional positional embedding
   2-dimensional positional embedding
   Relative positional embedding
7. 如果输入的图片信息大于 224x224，可以使用 2D 插值（interpolate 函数）

![](./images/2023-11-05-22-56-09.png)
![](./images/2023-11-05-23-03-06.png)

- 在相对较小的数据集上，ViT 不如 ResNet。在大数据集上，ViT 强于 ResNet。
- 混合模型（ResNet 得到的特征图交给 ViT）在小数据集上表现较好，在大数据集上表现趋近于 ViT。
  ![](./images/2023-11-05-23-14-05.png)
- 上面的图 a 展示了 patch embedding 如何进行的。这些东西跟卷积神经网络学到的很像，都像是 gabor filter。有一些基本的颜色和纹理。作者认为这些可以描述图像的底层结构。
- 图 b 展示了 position embedding。相似度越高的地方越黄。可以看到一维的 position embedding 已经学到了行和列，有了二维的概念。
  <img src="./images/2023-11-05-23-25-03.png" style="height:400px;" />
- 用输出的 token 折射回原来的输入图片，可以看出模型学到了跟分类相关的那些区域。

### ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision

时间：2021
被引用：874

&emsp;&emsp;ViLT 提出了一个极其简单的做多模态学习的框架结构。他把模态的特征抽取做到了极小化，把主要的计算量都放在了后面的模态融合中，大大提高了模型的推理速度。它的主要工作，是将目标检测从多模态学习的框架中给移除了。
![](./images/2023-10-16-17-55-42.png)
&emsp;&emsp;ViLT 一上来先展示了过去的方法都是怎么做的。他们一般在文本模态的处理极其简单，只做一个词嵌入。而图像方面则不然。他们要先过一遍 CNN 网络，再抽取区域性的特征，相当于做了目标检测的任务。这样做的缺点体现在时间中，视觉的时间占了 885/900ms，文本只占 15ms。而 VILT 使视觉的时间缩小到了 0.4ms，作为本文最大的卖点，同时它的准确度也没有掉太多。ViLT 写的有点综述的味道，在相关工作部分介绍的非常多，适合用来了解多模态学习。

that simply extracting input features requires much more computation than the multimodal interaction steps. 在单个模态花费的精力大多，而忽视了模态的融合。

To be fed into VLP models, image pixels need to be initially embedded in a dense form alongside language tokens.
不能把图像的像素丢给 transformer（序列太长）。因此要把图像的像素变化成语义化、离散化的特征。VIT(vision transformer)把图像分成 16\*16 的 patch。VIT 之前，大部分 VLP 的工作，靠的是目标检测（计算量太大）。

![](./images/2023-10-16-18-37-23.png)
single-stream approaches / dual-stream approaches
1：用一个模型处理两个输入，将两个序列直接连接(concatenation)。不管他们直接怎么交互，transformer 直接学。
2：两个模型先各自对各自的模态进行学习，最后融合。
dual-stream approaches 在准确率上只是稍微好一些，但是计算量更大，可以说他们半斤八两。

![](./images/2023-10-17-21-24-45.png)
灰色的表示属于哪个模态，绿色表示文字的位置，紫色表示图像的顺序。
image text matching loss / masked language modeling loss
如果把图像看做文本的话，那么输入全是文本，经过 transformer，最后完形填空，其实就是 NLP 领域的著名文章 BERT。如果不看文字部分其实就是 VIT。

文本小技巧：mask 单词/字母
图像小技巧：使用了图像领域公认好用的图像增强。为了防止图像增强后的图片跟文本不配对了，使用了 RandAugment 方法，但是不用其中的 color inversion 和 cutout，以保证图像和文本还能匹配在一起。

![](./images/2023-10-17-15-52-45.png)
&emsp;&emsp;ViLT 在 VQA NLVR 和 retrieval 这三个下游任务中进行了实验。在时间大大缩短的同时保持了一定程度的准确性。

- Visual Question Answer (VQA) 是对视觉图像的自然语言问答。
- Natural Language for Visual Reasoning（NLVR）：输入图像和描述，输出描述与图像的对应关系是否一致。
- video corpus moment retrieval 任务：在大量的视频中，根据查询语句检索得到相关的片段。

![](./images/2023-10-17-16-10-57.png)
&emsp;&emsp;ViLT 探究了 mask 文本、mask 图片和图像增强对结果的影响。mask 文本的效果十分一般。mask 图像的技术在 ViLT 的时候还不成熟（但在现在 MAE 中已经成熟），所以 ViLT 最终没有采用这种方式。图像增强带来的提升显著。

### ALBUF:Align before Fuse: Vision and Language Representation Learning with Momentum Distillation

时间：2022

![](./images/2023-10-18-01-50-36.png)
MOCO:Momentum Contrast for Unsupervised Visual Representation Learning
AlBUF 分为三个结构。一个是视觉编码器，一个文本编码器，一个多模态编码器。图像进入 transformer encoder，它一共有 N 层。文本进入文本的编码器，它有 L 层。在得到图像和文本之后先做一个 ITC(Image-Text Contrastive Loss)，这个对比学习的 Loss，去把图像文本的特征学好。然后文本特征通过 self attention 层，图像特征通过 cross attention 层，他们之间融合。最后通过 N-L 层的多模态编码器，得到多模态的特征。

### VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

时间：2022
![](./images/2023-10-18-02-10-01.png)
根据下游任务灵活切换。训练的时候是一个模型，推理的时候，根据现在输入的数据，决定应该使用什么样的模型结构
只有一个网络，自注意力层所有模态共享参数，但是在 feed forward 层，每个模态对应自己不同的 expert。

##### 词嵌入 Word Embedding

- 将词汇表中的词语转成固定长度的向量。将 One-Hot 编码表示的高维稀疏向量，转为低维连续的向量。
- 可以表达语义的相似性：
  <img src=./images/2023-10-16-18-24-20.png style="zoom:25%;" />
- 也可以描述词语之间的语义关联
  <img src=./images/2023-10-16-18-23-54.png style="zoom:25%;" />
- 词嵌入需要一个训练好的通用的嵌入矩阵
  ![](./images/2023-10-16-18-25-22.png)
- 当需要使用的时候，例如将“我喜欢学习数学”转成矩阵。此时我们先对句子进行切词。然后将得到的词转成 One-Hot 样式的矩阵。最后与词嵌入矩阵相乘，即可得到转化后的矩阵。
  ![](./images/2023-10-16-18-25-56.png)
